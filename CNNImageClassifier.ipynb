{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HU_IdRUI9SVd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgwgBiOQ90su",
        "outputId": "60346995-edff-49f2-8f7f-faa28f940b21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = \"/content/drive/MyDrive/Graduate_DeepL/cinic-10\"\n",
        "print(\"Root contents: \", os.listdir(data_root))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCS7rPS95go",
        "outputId": "91d87380-5a38-4947-9cb2-3cd5c255a276"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root contents:  ['valid', 'train', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    # Resize the input\n",
        "    transforms.Resize((32, 32)),\n",
        "    # Scale the data to [0.0, 1.0]\n",
        "    transforms.ToTensor(),\n",
        "    # Shift and scale the pixel values to [-1.0, 1.0]\n",
        "    transforms.Normalize([0.47889522, 0.47227842, 0.43047404], [0.24205776, 0.23828046, 0.25874835]),\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "#\n",
        "# train_ds = datasets.ImageFolder(os.path.join(data_root, \"train\"), transform=transform)\n",
        "# val_ds   = datasets.ImageFolder(os.path.join(data_root, \"valid\"), transform=transform)\n",
        "# test_ds  = datasets.ImageFolder(os.path.join(data_root, \"test\"),  transform=transform)\n"
      ],
      "metadata": {
        "id": "iLHBVh8J97kM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_ds = datasets.ImageFolder(os.path.join(data_root, \"train\"), transform=transform)\n",
        "full_val_ds   = datasets.ImageFolder(os.path.join(data_root, \"valid\"), transform=transform)\n",
        "full_test_ds  = datasets.ImageFolder(os.path.join(data_root, \"test\"),  transform=transform)\n"
      ],
      "metadata": {
        "id": "fSFH6buk-T8r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subset(dataset, fraction=0.1):\n",
        "    # Use dataset.targets if available (much faster than indexing each sample)\n",
        "    if hasattr(dataset, \"targets\"):\n",
        "        print(\"yes\")\n",
        "        targets = np.array(dataset.targets)\n",
        "    else:\n",
        "        targets = np.array([dataset[i][1] for i in range(len(dataset))])  # fallback\n",
        "\n",
        "    classes = np.unique(targets)\n",
        "    indices = []\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_indices = np.where(targets == cls)[0]\n",
        "        np.random.shuffle(cls_indices)\n",
        "        take = int(len(cls_indices) * fraction)\n",
        "        indices.extend(cls_indices[:take])\n",
        "\n",
        "    np.random.shuffle(indices)  # shuffle across classes\n",
        "    return Subset(dataset, indices)\n"
      ],
      "metadata": {
        "id": "wYVptQrW-WHc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = get_subset(full_train_ds, fraction=0.25)\n",
        "test_ds  = get_subset(full_test_ds, fraction=0.20)\n",
        "val_ds  = get_subset(full_val_ds, fraction=0.20)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy-8Uf46B5Zs",
        "outputId": "3a893268-4dfc-40f9-bf0f-4e10827ce32c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n",
            "yes\n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "FfjsL3ae-Yjb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(full_train_ds.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI1i11tY-aLR",
        "outputId": "2b71cfe2-a45f-4c9f-d7f9-ac061510310b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv Layer 1\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # input: [B, 3, 32, 32]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [B, 32, 16, 16]\n",
        "\n",
        "            # Conv Layer 2\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # [B, 64, 16, 16]\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # [B, 64, 8, 8]\n",
        "\n",
        "            # Conv Layer 3\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), # [B, 128, 8, 8]\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)   # [B, 128, 4, 4]\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),                           # [B, 128*4*4] = [B, 2048]\n",
        "            nn.Linear(128*4*4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pbR7ny-s-syK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(num_classes=num_classes).to(device)\n"
      ],
      "metadata": {
        "id": "r9jeHv3T_8Ye"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "gNruMPm-AIZ9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    running_loss, running_corrects, total = 0, 0, 0\n",
        "    for x, y in loader:\n",
        "    # for x, y in tqdm(loader, desc=\"train\", leave=False):\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      out = model(x)\n",
        "      loss = criterion(out, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item() * x.size(0)\n",
        "      _, preds = torch.max(out, 1)\n",
        "      running_corrects += (preds == y).sum().item()\n",
        "      total += x.size(0)\n",
        "    return running_loss/total, running_corrects/total"
      ],
      "metadata": {
        "id": "-mYYbDnnAYsL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    running_loss, running_corrects, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "      for x, y in loader:\n",
        "      # for x, y in tqdm(loader, desc=\"eval\", leave=False)\n",
        "          x, y = x.to(device), y.to(device)\n",
        "          out = model(x)\n",
        "          loss = criterion(out, y)\n",
        "          running_loss += loss.item() * x.size(0)\n",
        "          _, preds = torch.max(out, 1)\n",
        "          running_corrects += (preds == y).sum().item()\n",
        "          total += x.size(0)\n",
        "    return running_loss/total, running_corrects/total\n",
        "\n"
      ],
      "metadata": {
        "id": "WTBcNzaUAckG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"working\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader)\n",
        "    val_loss, val_acc = eval_epoch(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "\n",
        "test_loss, test_acc = eval_epoch(model, test_loader)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZCKdFsQAgJx",
        "outputId": "3aa5bcaf-2893-40fe-cadc-9d12bba459cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working\n"
          ]
        }
      ]
    }
  ]
}